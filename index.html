<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Andrea Palazzi</title>

    <meta name="author" content="Andrea Palazzi">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Andrea Palazzi</name>
                        </p>
                        <p>
                            I am a senior engineer at <a
                                href="https://www.nomitri.com/">Nomitri</a>, where I work on
                            Deep Learning applied to Computer Vision.
                        </p>
                        <p>
                            I got my Ph.D. at <a
                                href="http://imagelab.ing.unimore.it/imagelab/index.asp">AimageLab</a>,
                            at the University of Modena and Reggio Emilia, in Italy. I worked under
                            the supervision of <a
                                href="http://imagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Prof.
                            Rita Cucchiara</a>.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:andrea.palazzi@nomitri.com">Email</a> &nbsp/&nbsp
                            <a href="data/resume.pdf">CV</a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=soB7sYIAAAAJ&hl=en">Google
                                Scholar</a> &nbsp/&nbsp
                            <a href="https://github.com/ndrplz">Github</a> &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/andrea-palazzi">LinkedIn</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo"
                             src="images/me_circle.jpg" class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <!----------------------------------------------------------------------------------->
            <!------------------------------------ WORK ----------------------------------------->
            <!----------------------------------------------------------------------------------->

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Work (under construction)</heading>
                    </td>
                </tr>
                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='nomitri_image'>
                                <img src='images/nomitri_logo.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Senior Deep Learning Engineer</papertitle>
                        <p>
                            
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!----------------------------------------------------------------------------------->
            <!---------------------------------- RESEARCH --------------------------------------->
            <!----------------------------------------------------------------------------------->

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Research</heading>
                        <p>
                            During my PhD I mainly worked on a variety of topics, including
                            driverâ€™s gaze prediction, image and video saliency, synthetic data,
                            differentiable rendering, object pose estimation and image generation.
                            Representative papers are <span class="highlight">highlighted</span>.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <!------------------------------ Publication ------------------------------------>

                <tr bgcolor="#ffffd0">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='warp_and_learn_image'>
                                <img src='images/warp_and_learn.png'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/pdf/1907.10634.pdf">
                            <papertitle>Warp and Learn: Novel Views Generation for
                                Vehicles and Other Objects
                            </papertitle>
                        </a>
                        <br>
                        <strong>Andrea Palazzi</strong>,
                        Luca Bergamini,
                        Simone Calderara,
                        Rita Cucchiara
                        <br>
                        <em> IEEE Transactions on Pattern Analysis and Machine Intelligence
                            (TPAMI) </em>, 2020
                        <br>
                        <a href="https://arxiv.org/pdf/1907.10634">arXiv</a> &nbsp/&nbsp
                        <a href="https://github.com/ndrplz/semiparametric">code</a>
                        &nbsp/&nbsp
                        <a href="data/warp_and_learn.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p>
                            Self-supervised, semi-parametric approach for synthesizing novel views
                            of a vehicle starting from a single monocular image. Differently from
                            parametric (i.e. entirely learning-based) methods, we show how a-priori
                            geometric knowledge about the object and the 3D world can be integrated
                            into a deep learning based image generation framework.
                        </p>
                    </td>
                </tr>

                <!------------------------------ Publication ------------------------------------>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='learning_to_detect_image'>
                                <img src='images/learning_to_detect.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Matteo_Fabbri_Learning_to_Detect_ECCV_2018_paper.html">
                            <papertitle>Learning to Detect and Track Visible and Occluded Body
                                Joints in a Virtual World
                            </papertitle>
                        </a>
                        <br>
                        Matteo Fabbri,
                        Fabio Lanzi,
                        Simone Calderara,
                        <strong>Andrea Palazzi</strong>,
                        Roberto Vezzani,
                        Rita Cucchiara
                        <br>
                        <em>European Conference on Computer Vision (ECCV)</em>, 2018
                        <br>
                        <a href="https://arxiv.org/abs/1803.08319">arXiv</a> &nbsp/&nbsp
                        <a href="http://imagelab.ing.unimore.it/jta">dataset</a>
                        &nbsp/&nbsp
                        <a href="https://github.com/fabbrimatteo/JTA-Dataset">code (dataset)</a>
                        &nbsp/&nbsp
                        <a href="https://github.com/fabbrimatteo/JTA-Mods">code (GTAV mod)</a>
                        &nbsp/&nbsp
                        <a href="https://www.youtube.com/watch?v=9Q1UYzUysUk">video</a> &nbsp/&nbsp
                        <a href="data/learning_to_detect.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p>To overcome the lack of surveillance data with tracking, body part and
                            occlusion annotations we exploit the photo-realism of modern videogames
                            to create a vast Computer Graphics dataset (~500.000 frames, ~ 10
                            million body poses) for people tracking in urban scenarios.
                        </p>
                    </td>
                </tr>

                <!------------------------------ Publication ------------------------------------>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='end_to_end_6_dof_image'>
                                <img src='images/end_to_end_6_dof.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ieeexplore.ieee.org/document/8375682">
                            <papertitle>End-to-end 6-DoF Object Pose Estimation through
                                Differentiable Rasterization
                            </papertitle>
                        </a>
                        <br>
                        <strong>Andrea Palazzi</strong>,
                        Luca Bergamini,
                        Simone Calderara,
                        Rita Cucchiara
                        <br>
                        <em> European Conference on Computer Vision (ECCV) Workshops </em>, 2018
                        <br>
                        <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Palazzi_End-to-end_6-DoF_Object_Pose_Estimation_through_Differentiable_Rasterization_ECCVW_2018_paper.pdf">arXiv</a>
                        &nbsp/&nbsp
                        <a href="https://github.com/ndrplz/differentiable-renderer">code</a>
                        &nbsp/&nbsp
                        <a href="data/end_to_end_6_dof.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p> We introduce an approximated differentiable renderer to refine a
                            6-DoF pose prediction using only 2D alignment information. </p>
                    </td>
                </tr>

                <!------------------------------ Publication ------------------------------------>

                <tr bgcolor="#ffffd0">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='dreyeve_image'>
                                <img src='images/dreyeve.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ieeexplore.ieee.org/document/8375682">
                            <papertitle>Predicting the Driver's Focus of Attention: the DR(eye)VE
                                Project
                            </papertitle>
                        </a>
                        <br>
                        <strong>Andrea Palazzi</strong>,
                        <a href="http://www.davideabati.info/">Davide Abati</a>
                        Francesco Solera,
                        Simone Calderara,
                        Rita Cucchiara
                        <br>
                        <em> IEEE Transactions on Pattern Analysis and Machine Intelligence
                            (TPAMI) </em>, 2018
                        <br>
                        <a href="https://arxiv.org/abs/1705.03854">arXiv</a> &nbsp/&nbsp
                        <a href="http://aimagelab.ing.unimore.it/dreyeve">dataset</a> &nbsp/&nbsp
                        <a href="https://github.com/ndrplz/dreyeve">code</a> &nbsp/&nbsp
                        <a href="https://www.youtube.com/watch?v=GKjzOcwoc68">video</a> &nbsp/&nbsp
                        <a href="data/dreyeve.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p>We introduce a dataset of human fixations while driving, and a model to
                            predict them given an urban scene.</p>
                    </td>
                </tr>

                <!------------------------------ Publication ------------------------------------>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='birdseye_image'>
                                <img src='images/birdseye_data.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-319-68560-1_21">
                            <papertitle>Learning to Map Vehicles into Birdâ€™s Eye View</papertitle>
                        </a>
                        <br>
                        <strong>Andrea Palazzi</strong>,
                        <a href="http://www.guidoborghi.it/">Guido Borghi</a>,
                        <a href="http://www.davideabati.info/">Davide Abati</a>
                        Simone Calderara,
                        Rita Cucchiara
                        <br>
                        <em>International Conference on Image Analysis and Processing</em>, 2017
                        <br>
                        <font color="red"><strong>Best paper honorable mention</strong></font>
                        <br>
                        <a href="https://arxiv.org/abs/1706.08442">arXiv</a> &nbsp/&nbsp
                        <a href="http://imagelab.ing.unimore.it/imagelab/page.asp?IdPage=19">dataset</a>
                        &nbsp/&nbsp
                        <a href="https://github.com/ndrplz/surround_vehicles_awareness">code</a>
                        &nbsp/&nbsp
                        <a href="https://www.youtube.com/watch?v=t2mXv9j6LNw">video</a> &nbsp/&nbsp
                        <a href="data/birdseye.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p>A dataset with matched localization of vehicles from both camera car and
                            birdseye view, created from computer games. And a baseline model for
                            mapping locations across views.</p>
                    </td>
                </tr>

                <!------------------------------ Publication ------------------------------------>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='learning_where_to_attend_image'>
                                <img src='images/learning_where_to_attend.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ieeexplore.ieee.org/document/7995833">
                            <papertitle>Learning Where to Attend Like a Human Driver</papertitle>
                        </a>
                        <br>
                        <strong>Andrea Palazzi</strong>,
                        Francesco Solera,
                        Simone Calderara,
                        Stefano Alletto,
                        Rita Cucchiara
                        <br>
                        <em>Intelligent Vehicles Symposium</em>, 2017
                        <br>
                        <a href="https://arxiv.org/abs/1611.08215">arXiv</a> &nbsp/&nbsp
                        <a href="https://github.com/ndrplz/dreyeve">code</a> &nbsp/&nbsp
                        <a href="data/birdseye.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p>
                            We study the dynamics of the driver's gaze and use it as a proxy to
                            understand related attentional mechanisms. First, we build our analysis
                            upon two questions: where and what the driver is looking at? Second,
                            we model the driver's gaze by training a coarse-to-fine convolutional
                            network on short sequences extracted from the DR(eye)VE dataset.
                        </p>
                    </td>
                </tr>

                <!------------------------------ Publication ------------------------------------>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='dreyeve_dataset_image'>
                                <img src='images/dreyeve_dataset.jpg'>
                            </div>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ieeexplore.ieee.org/document/7789504">
                            <papertitle>DR(eye)VE: A Dataset for Attention-Based Tasks with
                                Applications to Autonomous and Assisted Driving
                            </papertitle>
                        </a>
                        <br>
                        Stefano Alletto,
                        <strong>Andrea Palazzi</strong>,
                        Francesco Solera,
                        Simone Calderara,
                        Rita Cucchiara
                        <br>
                        <em>CVPR Workshops</em>, 2016
                        <br>
                        <a href="https://openaccess.thecvf.com/content_cvpr_2016_workshops/w3/papers/Alletto_DREyeVe_A_Dataset_CVPR_2016_paper.pdf">arXiv</a>
                        &nbsp/&nbsp
                        <a href="https://github.com/ndrplz/dreyeve">code</a> &nbsp/&nbsp
                        <a href="data/dreyeve_dataset.bib">bibtex</a>
                        <br>
                        <p></p>
                        <p>
                            We propose a novel and publicly available dataset acquired during
                            actual driving. Our dataset, composed by more than 500,000 frames,
                            contains driversâ€™ gaze fixations and their temporal integration
                            providing task-specific saliency maps. Geo-referenced locations,
                            driving speed and course complete the set of released data.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            <a href="https://jonbarron.info/">I like this website.</a>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
